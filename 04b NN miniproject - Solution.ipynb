{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "\n",
    "## Week 4. Neural Nets Mini-project\n",
    "\n",
    "### Objectives    \n",
    "\n",
    "- Understand the fundamentals of neural networks  \n",
    "- Build simple perceptrons \n",
    "- Train a perceptron model with `scikit-learn` (Optional)\n",
    "\n",
    "### Prerequisites   \n",
    "\n",
    " - You should have [numpy](http://www.scipy.org/scipylib/download.html) and [scikit-learn](http://scikit-learn.org) installed  \n",
    " - You should have some understanding of [Python classes and objects](https://docs.python.org/3/tutorial/classes.html). If you are not familar with these, here is an interactive [tutorial](https://www.learnpython.org/en/Classes_and_Objects).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [\"Artificial\" Neural Network (ANN)](https://en.wikipedia.org/wiki/Artificial_neural_network) is a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs. It is inspired by the way biological neural networks in the human brain. ANNs have generated a lot of excitement in the Machine Learning research and industry with great breakthroughs in the areas of speech recognition, computer vision, and natural language processing.  \n",
    "\n",
    "Neural neworks are typically organized in layers. Layers are made up of a number of interconnected **\"nodes\"** which contain an **\"activation function\"**. Patterns are presented to the network via the **\"input layer\"**, which passes through one or more **\"hidden layers\"** to be processed. The hidden layers then connects to an **\"output layer\"** to give the outputs. The image below shows the first and the simplest neural network, the so-called [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network), wherein connections between the units do not form a cycle.  \n",
    "\n",
    "<img src=\"./img/nn/nn.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Node (Single Neuron)   \n",
    "The basic unit in a neural network is the neuron, often called a \"node\" or \"unit\". It receives input from some other nodes, or from an external source and computes the output. Each input has an associated *weight (w)*, which is assigned based on its relative importance to other inputs. The node applies a non-linear activation function to the weighted sum of its inputs, as follows, $$f(w_1*x_1 + w_2*x_2 + b)$$. \n",
    "\n",
    "\n",
    "### Activation Function \n",
    "The activation function takes a single number input and performs a certain mathematical operation. Some commonly used activation functions include:  \n",
    "\n",
    "- [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function): taks a real-value input and gives a characteristic \"S\"-shaped curve with returned values between 0 and 1  \n",
    "- Tanh: takes a real-valued input and gives output in the range [-1, 1] \n",
    "- [ReLu (**RE**ctified **L**inear **U**nit)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)): takes a real-valued input and thresholds it at zero (replaces negative values with zero).   \n",
    "\n",
    "\n",
    "### Perceptron  \n",
    "[Perceptron](https://en.wikipedia.org/wiki/Perceptron) was invented in the 1950s and was one of the first artificial neural networks to be produced. \n",
    "\n",
    "- **Single Layer Perceptron** This is the simplest feedforward neural network and does not contain any hidden layer.  \n",
    "\n",
    "- **Multi Layer Perceptron** A Multi Layer Perceptron has one or more hidden layers. It is more useful than Single Layer Perceptons in terms of practical applications.\n",
    "\n",
    "In this notebook, we will be building both a Single Layer Percepton and a Multi Layer Percepton. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Build Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Single Layer Perceptron  \n",
    "\n",
    "A single Layer Perceptron maps a series of inputs to an output. Each input is assigned a certain weight and then some mapping is applied to determines the output of the perceptron.   \n",
    "\n",
    "<img src=\"./img/NN/SL_perceptron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a simple perceptron  \n",
    "We will compute the weights of the inputs by taking the dot product of the input vector and the weight vector. This number is also known as the strenth or the activity of the inputs. We will use a simple step function to map to the perceptron output. The step function takes in the strenth of the inputs and we will compare the strength to some predefined threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# In this exercise, you will add in code that decides whether a perceptron will fire based\n",
    "# on the threshold.  \n",
    "#\n",
    "# ----------\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron0(object):\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes in \n",
    "        @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "\n",
    "        # TODO: calculate the strength with which the perceptron fires.\n",
    "        strength = np.dot(inputs, self.weights)\n",
    "\n",
    "        # TODO: return 0 or 1 based on the threshold\n",
    "        return 1 if strength > self.threshold else 0\n",
    "\n",
    "        \n",
    "\n",
    "def test_0():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron0(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([ 1, -1])) == 0 # <threshold -> 0\n",
    "    assert p1.activate(np.array([-1,  1])) == 1 # >threshold -> 1\n",
    "    assert p1.activate(np.array([ 2, -1])) == 0 # =threshold -> 0\n",
    "    \n",
    "\n",
    "test_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTIONS: \n",
    "\n",
    "\n",
    "- Why do we use some threshold and step function rather than just outputting the weighted inputs (dot product)?  \n",
    "> **Answer:**   \n",
    "<p style=\"background-color:#ffa\">The threshold and step function introduces the non-linearity which turns our perceptron into a something that can model interesting functions</p>\n",
    "\n",
    "- What parameter is learnable in a perceptron, i.e., what can be modified to allow the perceptron to model an arbitrary function?   \n",
    "> **Answer:** \n",
    "<p style=\"background-color:#ffa\">\n",
    "</p>\n",
    "- What does the input to a network of perceptrons look like? \n",
    "\n",
    "    A) Tensor of weights  \n",
    "    B) Matrix of numerical values  \n",
    "    C) Matrix of classifcations  \n",
    "    D) Matrix of numerical values and classifications for each row.    \n",
    "> **Answer:**  \n",
    "\n",
    "\n",
    "- Are Neural Networks used for classification or regression?   \n",
    "> **Answer:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron update rule \n",
    "\n",
    "The update rule for perceptron is as follows:  \n",
    "\n",
    "$$ w(t + 1) = w(t) + (\\eta * (y_i - \\hat{y_i}(t)) * x_i $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule. Updates should be performed online, revising\n",
    "# the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "\n",
    "class Perceptron1(Perceptron0):\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        super(Perceptron1, self).__init__(*args)\n",
    "\n",
    "\n",
    "    def update(self, X, y, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in \n",
    "        @param X, a 2D array consisting of a LIST of inputs and \n",
    "        @param y, a 1D array consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and \n",
    "        @param eta, an optional learning rate.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # TODO: for each data point...\n",
    "        for row in range(X.shape[0]):\n",
    "           \n",
    "            # TODO: obtain the prediction for that point\n",
    "            predictions = self.activate(X[row])\n",
    "            \n",
    "            # TODO: update self.weights based on prediction accuracy, learning\n",
    "            self.weights = self.weights + eta*((y[row]-predictions)*X[row])\n",
    "        \n",
    "            \n",
    "def test_p1():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron1(np.array([1, 1, 1]), 0)\n",
    "    p1.update(np.array([[2, 0, -3]]), np.array([1]))\n",
    "    print p1.weights\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron1(np.array([1, 2, 3]), 0)\n",
    "    p2.update(np.array([[3, 2, 1], [4, 0, -1]]), np.array([0, 0]))\n",
    "    print p2.weights\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron1(np.array([3, 0, 2]),0)\n",
    "    p3.update(np.array([[2, -2, 4],[-1, -3, 2], [0, 2, 1]]), np.array([0, 1, 0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "    return True\n",
    "\n",
    "test_p1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi Layer Perceptron \n",
    "\n",
    "The simple single node perceptron can only separate the data linearly. Multi Layer Perceptron is more useful in practice. This class of networks consists of multiple layers of computational units. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a sigmoid or a reLU function as an activation function, but we will continue to use the step function we used earlier.\n",
    "\n",
    "<img src=\"./img/NN/Q_multilayer.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We define a matrix of weights for each layer and apply the activation function for each unit in the layer to calculate the output to the next layer. For the example shown above, there will be one weight matrix (2x3) applied to the inputs for input layer (3x1), followed by activation function. To calculate the effect of the second layer, we take the activations from the first layer and apply a second weight matrix (1x2) and the activation function to get the final out (1x1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: \n",
    "\n",
    "Given weights for the hidden layer [1, 1, -5] and [3, -4, 2], and weights for the output layer [2, -1], what will the network output if inputs are [1, 2, 3] (as shown by the figure above)? \n",
    "\n",
    "**Answer:**  \n",
    "You can try this by hand, use numpy or use the Perceptron0 class we defined above.\n",
    "<p style=\"background-color:#ffa\">**Note:** I did this using numpy so you can see the guts of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Middle (hidden) layer hastwo perceptrons, each with three inputs and 1 output\n",
    "def activate(act):\n",
    "    for row in range(act.shape[0]):\n",
    "        strength = act[row][0]\n",
    "        act[row][0] = 1 if strength > 0 else 0\n",
    "    return act\n",
    "\n",
    "A = np.array([1, 1, -5, 3, -4, 2]).reshape(2, 3)\n",
    "inputs = np.array([1, 2, 3]).reshape(1, 3)\n",
    "activations = activate(np.dot(A, inputs.transpose()))\n",
    "B = np.array([2, -1]).reshape(1, 2)\n",
    "print(\"The final output should be {}\".format(activate(np.dot(B, activations)).ravel()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build an XOR network  \n",
    "\n",
    "**The XOR (exclusive OR) problem** is a problem that can be described with the logic table below, and visualised in input space: \n",
    "\n",
    "<img src=\"./img/NN/XOR.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "A two-layer neural network is capable of calculating XOR. The numbers within the neurons represent each neuron's explicit threshold (which can be factored out so that all neurons have the same threshold, usually 1). The numbers that annotate arrows represent the weight of the inputs. This net assumes that if the threshold is not reached, zero (not -1) is output.   \n",
    "In this example, let's build a network capable of modeling XOR funtion. The weights and thresholds are given below.  \n",
    "\n",
    "<img src=\"./img/NN/Q_XOR.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function based on the network above.\n",
    "#\n",
    "# You will need to create a network of perceptrons with the correct weights,\n",
    "# and define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "\n",
    "# Step 1: Set up the perceptron network\n",
    "# Use the Peceptron0 class we defined earlier. You will need two instances, one for each \"layer\"\n",
    "Network = [\n",
    "    # TODO: input layer, declare input layer perceptrons here\n",
    "    [Perceptron0(np.array([2, -1]), 1.5),\n",
    "     Perceptron0(np.array([-1, 2]), 1.5)\n",
    "    ],\n",
    "    # TODO: output node, declare output layer perceptron here\n",
    "    [Perceptron0(np.array([2, 2]), 1)]\n",
    "    ]\n",
    "\n",
    "# Step 2: Define a procedure to compute the output of the network, given inputs\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \"\"\"\n",
    "    Takes in \n",
    "    @param inputValues, a list of input values, and \n",
    "    @param Network, specifies a perceptron network. \n",
    "    @return the output of the Network for the given set of inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # TODO: calculate the OutputValue\n",
    "    innerlayer, outputlayer = Network\n",
    "    interim_results = np.zeros(len(innerlayer))\n",
    "    for i, p in enumerate(innerlayer):\n",
    "        interim_results[i] = p.activate(inputValues)\n",
    "    \n",
    "    return outputlayer[0].activate(np.array(interim_results))\n",
    "\n",
    "def test_xor():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    assert EvalNetwork(np.array([0, 0]), Network)==0\n",
    "           #\"(0, 0) should return 0 for XOR\") # 0 XOR 0 = 0 \n",
    "    assert EvalNetwork(np.array([0, 1]), Network)==1\n",
    "          # \"(0, 0) should return 0 for XOR\") # 0 XOR 1 = 1 \n",
    "    assert EvalNetwork(np.array([1, 0]), Network)==1\n",
    "           #\"(0, 0) should return 0 for XOR\") # 1 XOR 0 = 1 \n",
    "    assert EvalNetwork(np.array([1, 1]), Network)==0 # 1 XOR 1 = 0 \n",
    "    \n",
    "test_xor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Perceptron with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import perceptron\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make up some data\n",
    "data = pd.DataFrame.from_items([\n",
    "    ('Mass',   [10.0, 20.0, 4.6, 15.0, 2.0, 3.0, 3.0, 10.0, 15.0, 5.0]), \n",
    "    ('Length', [6.0,  5.0,  4.0, 4.4, 5.0, 4.5, 6.0, 7.0,  8.0,  9.0]),\n",
    "    ('Class',  [0, 0, 1, 1, 1, 1, 0, 0, 0, 0])])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: plot data using Length as x-axis and Mass as y-axis \n",
    "# and use color to distinguish two classes\n",
    "\n",
    "class_colors = np.array(['b', 'r'])\n",
    "plt.scatter(x=data.Length, y=data.Mass, c=class_colors[data.Class], alpha=0.6, s=50)\n",
    "plt.ylabel('Mass')\n",
    "plt.xlabel('Length')\n",
    "plt.ylim([0, 30])\n",
    "plt.xlim([0, 15]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a perceptron model \n",
    "# Create perceptron object \n",
    "p = perceptron.Perceptron(n_iter=1000, verbose=0, \\\n",
    "                          random_state=42, fit_intercept=True, eta0=0.001)\n",
    "\n",
    "# Train the perceptron object \n",
    "p.fit(data[['Length', 'Mass']], data['Class'])\n",
    "\n",
    "# Print the weights and bias\n",
    "weights = p.coef_\n",
    "bias = p.intercept_\n",
    "print \"weights = \" + str(weights)\n",
    "print (\"bias = \" + str(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make prediction for new data (length 11 and mass 12)\n",
    "prediction = p.predict([[11, 12]])\n",
    "print prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: plot decision boundary for the training data\n",
    "x_s = np.linspace(0, 15, 100) # an array of x_s\n",
    "y_s = - (x_s * weights[0][0] + bias) / weights[0][1]\n",
    "\n",
    "plt.scatter(data.Length, data.Mass, c=class_colors[data.Class], s=25)\n",
    "plt.plot(x_s, y_s, 'k--') # the decision boundary\n",
    "plt.ylabel('Mass')\n",
    "plt.xlabel('Length')\n",
    "plt.ylim([0, 30])\n",
    "plt.xlim([0, 15]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
