{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 07a: Unsupervised Learning - Comparing Different Clustering Algorithms\n",
    "\n",
    "\n",
    "Last week, we used k-means clustering to the Enron financial dataset available in the [ud120-projects repo](https://www.github.com/udacity/ud120-projects). In today's lesson, we will work through a few other clustering algorithms and consider scenarios where one method is preferred over the others. The examples and code are drawn from the [sklearn documentation](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)\n",
    "\n",
    "We will look at the results of a few different clustering algorithms on \"interesting\" (or pathological) datasets with two independent variables. These examples will give us some intuition about which clustering algorithm might perform better in a given scenario. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries \n",
    "As usual, we start by loading the packages we are going to use. \n",
    "\n",
    "**Note:** We will use a function to plot the results of hierarchical clustering (called dendrograms) which is not available in `sklearn`. For tht reason, we will need to also load scipy, and the scipy package will need to be loaded in the environment you are using for this class. \n",
    " - numpy\n",
    " - sklearn (this version of the notebook uses v0.18). \n",
    " - matplotlib\n",
    " - seaborn\n",
    " - scipy\n",
    "\n",
    "Run the cell below to load them\n",
    " \n",
    "If you get an error loading scipy, please make sure it is loaded into your conda environment _before_ you start the Jupyter  notebook server\n",
    "\n",
    "conda install -n mlnd scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python \n",
    "\n",
    "try:\n",
    "    import scipy\n",
    "    print(\"Successfully imported scipy! (Version {})\".format(scipy.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import scipy!\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"Successfully imported matplotlib! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"Successfully imported sklearn! (Version {})\".format(sklearn.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import sklearn!\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    from IPython.display import Image\n",
    "    print(\"Successfully imported display from IPython.display and Image!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1 | Load Datasets\n",
    "\n",
    "The idea of clustering is to find entities (records, rows) that are most similar to each other and group them together. All clustering algorithms rely on some measure of \"distance\" to determine similarity (sometimes called a dissimilarity measure). Many algorithms, e.g., the k-Means algorthm we used last time, uses Euclidean distance. Most clusterers in `sklearn` have a parameter for defining this distance. \n",
    "\n",
    "We will start by creating three toy datasets. Since these are datasets we are creating, we know the actual cluster assignments. This will be useful in \"scoring\" our algorithms later. In practice, though, we will not have labels for unsupervised learning and we will only be able to use some measure of cluster quality (such as silhouette score) to compare the results of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from ipython notebook from http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n",
    "import time\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "colors = np.hstack([colors] * 20)\n",
    "\n",
    "datasets = [noisy_circles, noisy_moons, blobs];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the algorithms are sensitive to distance by design, it is very important that all features (variables) are scaled evenly. In this section, we are using `StandardScaler` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'll just take one dataset and look at its statistics before and after scaling\n",
    "plt.figure(figsize=(4*len(datasets), 4));\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01);\n",
    "\n",
    "for num, data in enumerate(datasets):\n",
    "    X, y = data\n",
    "    XS = StandardScaler().fit_transform(X)\n",
    "\n",
    "    plt.subplot(1, 3, num+1)\n",
    "\n",
    "    plt.scatter(XS[:, 0], XS[:, 1], color=colors[y].tolist(), s=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 | Hierarchical Clustering\n",
    "\n",
    "In the video lectures on Single Link Clustering, Charles and Michael described how HACs are constructed. You also need to choose how to measure the distance between two points (or elements), a point and a cluster, as well as two clusters. `sklearn` provides a single class [_AgglomerativeClustering_](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)  which has both the point-wise distance metric (_affinity_) and the method for calculating cluster distance (_linkage_) as parameters, along with the number of clusters (`n_clusters`)\n",
    "\n",
    "_affinity_ takes on the default keyword `euclidean`. Other recognized values are `euclidean`, `l1`, `l2`, `manhattan`, `cosine`, or `precomputed`. If linkage is `ward`, only `euclidean` is accepted.\n",
    "\n",
    "_linkage_ takes on the default keyword `ward`. Pther recognized values are `complete`, and `average`. There is no option to use the single-link method in `sklearn`. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "\n",
    "`ward` minimizes the variance of the clusters being merged.\n",
    "`average` uses the average of the distances of each observation of the two sets.\n",
    "`complete` or maximum linkage uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "\n",
    "To determine which clustering result better matches the original labels of the samples, we can use ```adjusted_rand_score``` which is an *external cluster validation index* yielding a score between -1 and 1, where 1 means two clusterings are identical to their **true** grouping in a dataset (regardless of what label is assigned to each cluster).\n",
    "\n",
    "Let's import the clustering class and this metric [`adjusted_rand_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) for an indication of how well our clustering algorithm does with these generated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines a helper function that performs HA clustering using three linkage criteria for the dataset passed. Try it with each of the toy datasets we created to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_clusters(dataset, n_clusters):\n",
    "    X, y = dataset\n",
    "    normalized_X = StandardScaler().fit_transform(X)\n",
    "    ward = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    ward_pred = ward.fit_predict(normalized_X)\n",
    "\n",
    "    complete = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"complete\")\n",
    "    complete_pred = complete.fit_predict(normalized_X)\n",
    "\n",
    "    avg = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n",
    "    avg_pred = avg.fit_predict(normalized_X)\n",
    "\n",
    "\n",
    "    ward_ar_score = adjusted_rand_score(y, ward_pred)\n",
    "    complete_ar_score = adjusted_rand_score(y, complete_pred)\n",
    "    avg_ar_score = adjusted_rand_score(y, avg_pred)\n",
    "\n",
    "    print( \"Scores: \\nWard: {:5.4f}\\nComplete: {:5.4f}\\nAverage: {}\".format(ward_ar_score, complete_ar_score, avg_ar_score))\n",
    "\n",
    "    plt.figure(figsize=(3*len(datasets) +4, 4));\n",
    "    plt.subplots_adjust(left=.05, right=.95, bottom=.001, top=.96, wspace=0.15, hspace=.01);\n",
    "    plot_labels = (\"Ward\", \"Complete\", \"Average\")\n",
    "    for num, pred in enumerate((ward_pred, complete_pred, avg_pred)):\n",
    "        plt.subplot(1, 3, num+1)\n",
    "\n",
    "        plt.scatter(normalized_X[:,0], normalized_X[:,1], color=colors[pred].tolist(), s=10)\n",
    "        plt.text(.99, 0.05, plot_labels[num],\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "Perform clustering with the three toy data sets and three linkage criteria using clusters 2, 3, and 4. What was the best (highest ```adjusted_rand_score```) linkage type and number of clusters for the three datasets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(blobs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 | Dendrogram visualization with scipy\n",
    "\n",
    "It is also interesting to visualize how these clusters are built up. The cells below calculate and plot a \"dendrogram\" (an entity with many levels of branching). A key point to remember is that every horizontal line identifying two clusters that get merged has its own height (difficult to see because of the scaling). Once the complete tree structure as been created, you can draw a horizontal line at an appropriate height to get an arbitrary number of clusters (of course can't have more than the number of leaves you started with!)\n",
    "\n",
    "Let's visualize the clustering result for `blobs` using 3 clusters and the ward linkage method. \n",
    "\n",
    "To do that, we'll need to use Scipy's [```linkage```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) function to perform the clusterirng again so we can obtain the linkage matrix it will later use to visualize the hierarchy using [```dendrogram```](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html)\n",
    "\n",
    "The `height` (y-axis) at which the clusters are merged are scaled to the \"distance\" between the clusters. For the cleanly separated clusters in `blobs`, the dendrogram also shows a leanly delineated \"tree\" up to three clusters, then they begin to run together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import scipy's linkage and dendrogram functions to conduct the clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "X, y = blobs\n",
    "normalized_X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Specify the linkage type. Scipy accepts 'ward', 'complete', 'average', as well as other values\n",
    "# Pick the one that resulted in the highest Adjusted Rand Score\n",
    "linkage_type = 'ward'\n",
    "\n",
    "linkage_matrix = linkage(normalized_X, linkage_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/#Plotting-a-Dendrogram\n",
    "#\n",
    "# calculate and plot the dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Height')\n",
    "\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    truncate_mode='level',  # show only the highest p levels of merged clusters\n",
    "    p=5,  # show only the last p levels of merged clusters\n",
    "    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 | DBSCAN\n",
    "\n",
    "As you likely saw earlier, AgglomerativeClustering doesn't work too well for the ```noisy_moons``` or ```noisy_circles``` datasets. Let's try DBSCAN on the three now.\n",
    "\n",
    "[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) (Density-Based Spatial Clustering of Applications with Noise -- an algorithm not discussed in the lectures, but is well-researched and known to be fairly robust). Run he cell below to see the results of clustering with the default parameters using DBSCAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "plt.figure(figsize=(3*len(datasets) +4, 4));\n",
    "plt.subplots_adjust(left=.05, right=.95, bottom=.001, top=.96, wspace=0.15, hspace=.01);\n",
    "plot_labels = (\"noisy_circles\", \"noisy_moons\", \"blobs\")\n",
    "\n",
    "for num, dataset in enumerate(datasets):\n",
    "    X, y = dataset\n",
    "    normalized_X = StandardScaler().fit_transform(X)\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    pred = dbscan.fit_predict(normalized_X)\n",
    "    plt.subplot(1, 3, num+1)\n",
    "    ascore = adjusted_rand_score(y, pred)\n",
    "\n",
    "    plt.scatter(normalized_X[:,0], normalized_X[:,1], color=colors[pred].tolist(), s=10)\n",
    "    plt.text(.99, 0.05, plot_labels[num],\n",
    "             transform=plt.gca().transAxes, size=15,\n",
    "             horizontalalignment='right')\n",
    "    plt.text(.4, 0.95, \"score={:5.3f}\".format(ascore),\n",
    "             transform=plt.gca().transAxes, size=15,\n",
    "             horizontalalignment='right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "\n",
    "The default values of the two important parameters ```eps``` and ```min_samples``` doesn't work well for our chosen datasets. Try changing ```eps``` to see if you can get higher ```adjusted_rand_score``` for these datasets.\n",
    "\n",
    "**Answer:   **  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 | Gaussian Mixture Models and Expectation Maximization (EM)\n",
    "\n",
    "The mixture model is inherently different from the other clustering algorithms we used in that  it determines a probability that any given case belongs to one of the _n_ clusters that it was supposed to find rather than a hard (0 or 1) assignment to a cluster. This can be very helpful when the clusters are overlapping.\n",
    "\n",
    "Consider this simplified description of the mathematics behind _K_-means. We have a set of _N_ observations ($ x \\epsilon \\{1, ... , N\\} $), and the algorithm is tasked with finding the best assignment of these points to the _K_ clusters (as well as finding the cluster centers). Let's introduce an indicator variable $ z_{i,j} $ that associates a point $ x_i $ with center $ k \\varepsilon \\{1,..,K\\} $. $ z_{i,j} $ is zero for all combinations of $ i $ and $ j $ except when $ j = k $, where $k $ is the cluster to which $ x_i $ is assigned. The objective or cost function, $ J $ is the sum of squared distances (or mean squared distance if you want to scale it by the number of points) for this cluster assignment:\n",
    "\n",
    "$$\\large J = \\Sigma_{i=1}^{N} \\cdot \\Sigma_{j=1}^K z_{i,j} \\| x_i - \\mu_k \\|^2 $$\n",
    "\n",
    "The _K_-means algorithm therefore needs to find values of  $ z_{i,j} $ and $ \\mu_k $ that minimizes $ J $. There is closed form solution for this as the cost function is easily differentiable.\n",
    "\n",
    "In a gaussian mixture model, the \"points\" in our dataset are assumed to be drawn from one or more i.i.d. (independent and identically distributed) random variables. The algorithm needs to be supplied the number of these i.i.d.s (i.e., the number of clusters); it then determines the probability that a given point belongs to each of the clusters it identifies. In words (the words of Charles and Michael), we assume:\n",
    "- the data was generated by a underlying set of $ K $ i.i.d.s\n",
    "- each i.i.d. is characterized by the Normal distribution, i.e., $\\large \\mathcal{N}( x | \\mu_k, \\sigma_k) $\n",
    "- we don't know what the $ \\mu_k $ and $ \\sigma_k $ are (for simplicity we consider the $ \\sigma $ the same for all i.i.d.s\n",
    "- but we want to use our observations of each point $ x_n $ to deduce the the set $ \\{ \\mu_1, \\mu_2, ... , \\mu_K, \\sigma \\} $\n",
    "\n",
    "We want to determine a set of $ \\mu_k $ that maximizes the probability of observing the set of $ x_n $. However, unlike the _K_-means case, there isn't a closed form solution for solving this problem when there are more than one i.i.d involved. The Expectation Maximization algorithm achieves this goal by choosing some centers (randomly), calculating the resulting set of $ z_{i,j} $, i.e., cluster assignments, that maximizes the probabiity that the dataset was derived from this set of i.i.d.s. It then calculates the centers from these $ z_{i,j} $, iterating until the centers stop changing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Implementation_\n",
    "There are two things to note:\n",
    " - GaussianMixture is in the ```sklearn.mixture``` (not ```sklearn.cluster```) package\n",
    " - it does not have a fit_predict function, so we have to fit first, then use the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "plt.figure(figsize=(3*len(datasets) +4, 4));\n",
    "plt.subplots_adjust(left=.05, right=.95, bottom=.001, top=.96, wspace=0.15, hspace=.01);\n",
    "plot_labels = (\"noisy_circles\", \"noisy_moons\", \"blobs\")\n",
    "\n",
    "for num, dataset in enumerate(datasets):\n",
    "    X, y = dataset\n",
    "    normalized_X = StandardScaler().fit_transform(X)\n",
    "    G = GaussianMixture(n_components=1).fit(normalized_X)\n",
    "    pred = G.predict(normalized_X)\n",
    "    plt.subplot(1, 3, num+1)\n",
    "    ascore = adjusted_rand_score(y, pred)\n",
    "\n",
    "    plt.scatter(normalized_X[:,0], normalized_X[:,1], color=colors[pred].tolist(), s=10)\n",
    "    plt.text(.99, 0.05, plot_labels[num],\n",
    "             transform=plt.gca().transAxes, size=15,\n",
    "             horizontalalignment='right')\n",
    "    plt.text(.4, 0.95, \"score={:5.3f}\".format(ascore),\n",
    "             transform=plt.gca().transAxes, size=15,\n",
    "             horizontalalignment='right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE: ** Try different values of ```n_components``` to find values that maximize the ```adjusted_rand_score```. Why does GMM not work well for the ```noisy_moons``` and ```noisy_circles``` datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_** Answer **_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
