{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 06: Unsupervised Learning - Clustering\n",
    "\n",
    "## K-Means Clustering - Cluster Visualization\n",
    "\n",
    "#### Clustering Mini Project \n",
    "\n",
    "In this project, we’ll apply [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) (a variant of [Lloyd's Algorithm](https://en.wikipedia.org/wiki/Lloyd's_algorithm) or Voronoi Iteration) to the Enron financial dataset available in the [ud120-projects repo](https://www.github.com/udacity/ud120-projects). For your convenience, I have included the data file in this repository. We're also going to define a function `DrawClusters()` that will allow us to visualize the resulting clusters. \n",
    "\n",
    "Our final goal, of course, is to identify persons of interest; since we have labeled data, this is not a question that particularly calls for an unsupervised approach like k-means clustering. Nonetheless, it is interesting to attempt a clustering analysis to see how well this kind of approach can distinguish between poi's and non-poi's. You’ll also get some hands-on practice with k-means in this project, and play around with feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 | Preliminaries \n",
    "As usual, we start by loading the packages we are going to use. These should be already by installed in your environment. There are no new packages that need to be installed for this session\n",
    "\n",
    " - numpy\n",
    " - pandas\n",
    " - sklearn\n",
    " - matplotlib\n",
    " \n",
    "In addition, we will be using data and some code from the ud120-projects repo, so those should be available on your local machine as well\n",
    " \n",
    " Run the cell below to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python \n",
    "\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"Successfully imported matplotlib! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "import os, sys\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    from IPython.display import Image\n",
    "    print(\"Successfully imported display and Image from IPython.display!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import from IPython.display\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 | Load the Dataset\n",
    "\n",
    "**Run** he next two cells to load he dataset and define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Change the path to the folder containing the enron dataset if it is not is the same relative location as in\n",
    "### the ConnectIntensive repo. Set the correct path in PATH_TO_MINI \n",
    "###\n",
    "### Once this is correctly set using either a relative path or an absolute path, the rest of the code should work correctly\n",
    "\n",
    "PATH_TO_MINI = \"./data\"\n",
    "\n",
    "### Once you have the path set, the output from this cell should say so\n",
    "\n",
    "try:\n",
    "    path_ok = os.path.isfile(os.path.join(PATH_TO_MINI,\"clustering\",\"final_project_dataset.pkl\"))\n",
    "    if not path_ok:\n",
    "        raise Exception(\"Path is not set correctly\")\n",
    "    print \"PATH_TO_MINI appears correct (can open project data file)\" \n",
    "except Exception as e:\n",
    "    print e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines the `DrawClusters()` function adapated from  [the `Draw()` function from `\"ud120-projects/kmeans/k_means_cluster.py\"`](https://github.com/udacity/ud120-projects/blob/master/k_means/k_means_cluster.py). Here's a quick summary of changes:\n",
    "  - `DrawClusters()` works with the `pandas` `DataFrame` object `enron_df`.\n",
    "  - Some of the functionality from `\"ud120-projects/tools/feature_format.py\"` is incorporated into `DrawClusters()`:\n",
    "    - The ability to remove instances with **all** zeroes from k-means clustering\n",
    "    - The ability to remove instances with **any** zeroes from k-means clustering\n",
    "    - The ability to rescale features, remapping them to the interval [0,1]\n",
    "  - `DrawClusters()` also draws the resulting decision boundary when `feature_list` contains only two features.\n",
    "  \n",
    "**Read** through the code for `DrawClusters` to see how `KMeans` is used within the function to compute and display the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PATH_TO_MINI)\n",
    "sys.path.append(os.path.join(\"./tools\"))\n",
    "sys.path.append(os.path.join(\"./k_means\"))\n",
    "\n",
    "## Adapted from ud120-projects/k_means/k_means_cluster.py\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def DrawClusters(n_clusters = 2,\\\n",
    "                 feature_list = [\"salary\",\"exercised_stock_options\"],\\\n",
    "                 remove_any_zeroes = False,\\\n",
    "                 remove_all_zeroes = True,\\\n",
    "                 rescale_features = False,\\\n",
    "                 x_label = 'Salary',\\\n",
    "                 y_label = 'Exercised Stock Options'):\n",
    "    \n",
    "    '''\n",
    "    Plots k-means clusters trained on a list of features from enron_df\n",
    "    @param n_clusters:        an integer, the number of clusters to form.\n",
    "    @param feature_list:      a list of strings, the features to use in KMeans clustering\n",
    "                              (first two features in the list will be plotted on x,y axes)\n",
    "                              if len(feature_list) == 2, decision boundaries will be plotted \n",
    "    @param remove_any_zeroes: a boolean, whether or not to remove points when clustering\n",
    "                              that contain ANY zeroes\n",
    "    @param remove_all_zeroes: a boolean, whether or not to remove points when clustering\n",
    "                              that contain ONLY zeroes\n",
    "    @param rescale_features:  a boolean, whether or not to rescale features from 0 to 1\n",
    "    @param kmns:              (optional) KMeans cluster model. If provided, ignores \n",
    "    @param x_label:           a string, the x-axis label of the plot\n",
    "    @param y_label:           a string, the y-axis label of the plot\n",
    "    '''\n",
    "\n",
    "    # Initialize color and shape lists for scatterplot\n",
    "    # Note: need to lengthen the color and shape lists if\n",
    "    #       you want to use more than 7 clusters.\n",
    "    color_list = [\"r\",\"g\",\"b\",\"c\",\"m\",\"y\",\"k\"]\n",
    "    shape_list = [\"^\",\"o\",\"s\",\"v\",\"D\",\"<\",\"h\"]\n",
    "    \n",
    "    # Initialize figure and axes:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # We are going to cluster the data using KMeans,\n",
    "    # based on the features from the parameter feature_list.\n",
    "    X = enron_df[feature_list]\n",
    "    \n",
    "    # We can see if clustering correctly identified POIs\n",
    "    y = enron_df['poi']\n",
    "    \n",
    "    # If desired, remove instances where ANY feature is zero\n",
    "    if remove_any_zeroes:\n",
    "        X = X[~(X.T == 0).any()]\n",
    "        \n",
    "    # If desired, remove instances where ALL features are zeroes\n",
    "    if remove_all_zeroes:\n",
    "        X = X[~(X.T == 0).all()]\n",
    "\n",
    "    # If desired, rescale features\n",
    "    if rescale_features:\n",
    "        for feature in X.columns:\n",
    "            min_feature = X[~(X[feature] == 0)][feature].min()\n",
    "            max_feature = X[~(X[feature] == 0)][feature].max()\n",
    "            range_feature = max_feature - min_feature\n",
    "            X[feature] = (X[feature] - min_feature)*1.0 / range_feature\n",
    "            \n",
    "    # If we removed any instances above, we need to keep \n",
    "    # only the corresponding instances in y (the POI Series)\n",
    "    y = y.loc[X.index]\n",
    "    \n",
    "    # By default, the first two features in the list are chosen\n",
    "    # as the x and y features.\n",
    "    x_feature = feature_list[0]\n",
    "    y_feature = feature_list[1]\n",
    "\n",
    "    # Determine the min & max values of x_feature, compute the range,\n",
    "    # and pad the minimum and maximum x values by 5% of the range\n",
    "    x_min, x_max = X[x_feature].min(), X[x_feature].max()\n",
    "    x_range = x_max - x_min\n",
    "    x_min -= x_range * 0.05\n",
    "    x_max += x_range * 0.05\n",
    "    \n",
    "    # Determine the min & max values of y_feature, compute the range,\n",
    "    # and pad the minimum and maximum y values by 5% of the range\n",
    "    y_min, y_max = X[y_feature].min(), X[y_feature].max()\n",
    "    y_range = y_max - y_min\n",
    "    y_min -= y_range * 0.05\n",
    "    y_max += y_range * 0.05\n",
    "    \n",
    "    # Compute k-means clustering.\n",
    "    kmns = KMeans(n_clusters = n_clusters).fit(X)\n",
    "\n",
    "    \n",
    "    # We can visualize the decision boundaries if the \n",
    "    # k-means clusters are formed from just two features\n",
    "    if len(feature_list) == 2:\n",
    "        # Return coordinate matrices xx and yy from arrays\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, num = 500),\\\n",
    "                             np.linspace(y_min, y_max, num = 500))\n",
    "\n",
    "        # Use the clustering to make a prediction for each point\n",
    "        # in the coordinate matrices, then reshape for plotting.\n",
    "        Z = kmns.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot semitransparent filled contours for the\n",
    "        # cluster decision boundaries\n",
    "        ax.contourf(xx, yy, Z,\\\n",
    "                    levels=list(np.arange(n_clusters+1)-0.5),\\\n",
    "                    colors=tuple(color_list[:n_clusters]),alpha=0.2)\n",
    "    \n",
    "    # Scatterplot all of the points in each cluster,\n",
    "    # with each cluster a different color and shape\n",
    "    pred = kmns.predict(X)\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        X_cluster = X[pred == cluster_idx]\n",
    "        ax.scatter(X_cluster[x_feature], X_cluster[y_feature],\\\n",
    "                   s=30, marker = shape_list[cluster_idx],\\\n",
    "                   edgecolor='k', facecolor=color_list[cluster_idx], alpha=0.8)\n",
    "        \n",
    "    # Denote the centroids of each cluster with a white X\n",
    "    centers = kmns.cluster_centers_\n",
    "    if len(feature_list) == 2:\n",
    "        for center in centers:\n",
    "            ax.scatter(center[0],center[1],marker='x',facecolor=\"w\",linewidth=3,s=50)\n",
    "            \n",
    "    # Set the title and axes labels, and adjust the aspect ratio for rescaled data\n",
    "    if rescale_features:\n",
    "        title   = ax.set_title(\"K Means - {} Clusters (Rescaled)\".format(n_clusters))\n",
    "        # Force the axes aspect ratio to be plotted equally\n",
    "        ax.set(adjustable='box-forced', aspect='equal')\n",
    "    else:\n",
    "        title   = ax.set_title(\"K Means - {} Clusters\".format(n_clusters))\n",
    "    x_label = ax.set_xlabel(x_label)\n",
    "    y_label = ax.set_ylabel(y_label)\n",
    "    \n",
    "    # Set the x- and y-axis limits using the padded minimum and maximum values\n",
    "    xlim = ax.set_xlim((x_min,x_max))\n",
    "    ylim = ax.set_ylim((y_min,y_max))\n",
    "    \n",
    "print(\"DrawClusters() is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "\n",
    "enron_data = pickle.load( open(os.path.join(PATH_TO_MINI,\"clustering\",\"final_project_dataset.pkl\"), \"r\"))\n",
    "print \"Loaded dataset - has {} keys (or rows)\".format( len(enron_data.keys()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 | Data Exploration and pre-processing\n",
    "\n",
    "We want to get a sense of what is in the data set, whether there are any missing values that need to be cleaned up. \n",
    "\n",
    "We couldn't find *all* the details for each person involved with the Enron scandal, so the Enron data set has some missing values denoted by `\"NaN\"` entries in the data dictionary. Because we must pass numeric arrays into `sklearn` modules, we need to preprocess the data dictionary a bit before we can use [`sklearn.cluster.KMeans()`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
    "\n",
    "##### Question 1 - How many different persons are there in this dataset? Are there any keys that should be excluded? (HINT: There is at least one). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The preliminary structure we loaded is a python dictionary whose keys are the names of people at Enron.\n",
    "## print the list os keys to see all the names. Scroll through the names to see if you can identify one obvious key \n",
    "## that should be removed (it is not a person's name)\n",
    "\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add appropriate code in the cell below to complete our preprocessing steps:\n",
    "  - Remove the outlier: the bad entry is `\"TOTAL\"` in the data dictionary containing totals of all features.\n",
    "  - Create a pandas `DataFrame` object from the Enron data dictionary\n",
    "  - Take the **transpose** of the Enron `DataFrame` with [`DataFrame.T`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.T.html), so that rows correspond to individuals (instances). \n",
    "  - Impute missing values: Replace all \"NaN\" values in the DataFrame with **zeroes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the Enron dataset:\n",
    "# there's an outlier--remove it! \n",
    "enron_data.pop(\"TOTAL\", 0) # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As we've seen before, `pandas` dataframes provide a convenient way of managing data. We will use the `from_dict` method of a DataFrame object to load our python data_dict. However, when the dictionary object loaded, the resulting DataFrame has the list of keys as its columns. We can \"flip\" this using the transpose (.T) so that each row contains the data for one person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame object from the Enron data dictionary\n",
    "enron_df = None # TO DO - \n",
    "\n",
    "# Take the transpose (.T) of the Enron DataFrame and set it to itself\n",
    "# so that rows of the DataFrame correspond to individuals\n",
    "enron_df  #TO DO\n",
    "\n",
    "# Display the DataFrame to get a sense of the data\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, several of the features have missing values and we will need to get rid of them using imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change all entries in the DataFrame with \"NaN\" to zeroes.\n",
    "enron_df[enron_df == \"NaN\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question 2 - Why is it ok to replace the missing values with 0? Write your thoughts in the cell below and we will discuss as a group. Think about the other ways you can impute missing data, e.g., mean or median values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a scatterplot of the data with two features, then look at the resulting figure. What clusters you would expect to arise from this data if 2 clusters are created? Try different pairs of features and select three features. We will use them for creating clusters using the K-means algorithm later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(enron_df[\"salary\"],enron_df[\"exercised_stock_options\"])\n",
    "title   = ax.set_title(\"Exercised Stock Options vs. Salary\")\n",
    "xlabel  = ax.set_xlabel(\"Salary\")\n",
    "ylabel  = ax.set_ylabel(\"Exercised Stock Options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 3** - What are the features (i.e., columns or variables) available in this dataset? Which ones might be useful for identifying a `person of interest` (_poi_)? Please write down three choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Clustering\n",
    "Let's perform k-means clustering on the data using only two of the features you identified above. Enter them as the values of the variables `feature_1` and `feature_2`. The online quiz specifies which features to use  (\"salary\", \"exercised_stock_options\"), but try experimenting with other combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with two features\n",
    "\n",
    "**Run** the cell below to generate and visualize the results of `Kmeans` clustering with 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawClusters(n_clusters = 2,\\\n",
    "             feature_list = [feature_1, feature_2],\\\n",
    "             remove_any_zeroes = False,\\\n",
    "             remove_all_zeroes = True,\\\n",
    "             rescale_features = False,\\\n",
    "             x_label = feature_1.replace(\"_\", \" \"),\\\n",
    "             y_label = feature_2.replace(\"_\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** In the scatterplot that pops up, are the clusters what you expected? ** (_All answers are correct!_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three features clustering\n",
    "\n",
    "In this section add a third feature and rerun the clustering. The online quiz specifies which feature to use  (\"total payments\"), but feel free to experiment with others. \n",
    "\n",
    "You can copy and modify the cells from the two features groups above to complete this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - modify these variable to try other feature combinations\n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "feature_3 = \"total_payments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 - Compare the plot with the clusterings to the one you obtained with 2 input features. Do any points switch clusters? How many? This new clustering, using 3 features, couldn’t have been guessed by eye--it was the k-means algorithm that identified it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling \n",
    "\n",
    "As you can see, if you attempted to use one of the finance features as a variable, most of the \"points\" are close together but there are several that are fairly far away from the rest. Feature scaling is way to \"normalize\" the data so we don't end up emphasizing any one dimension too much.\n",
    "\n",
    "The next few questions (aligned with the online mini-project quizzes) lead you through some of the calculations that are done for feature scaling.\n",
    "\n",
    "\n",
    "(NB: if you look at finance_features, there are some \"NaN\" values that have been cleaned away and replaced with zeroes--so while those might look like the minima, it's a bit deceptive because they're more like points for which we don't have information, and just have to put in a number. So for this question, we will go back to unmodified data and look for the maximum and minimum numbers that show up there, ignoring all the \"NaN\" entries). To help with this task, run the cell below to create the data frame df from the enron_data dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame object from original the Enron data dictionary\n",
    "df = pd.DataFrame.from_dict(enron_data)\n",
    "\n",
    "# Take the transpose (.T) of the Enron DataFrame,\n",
    "# so that rows of the DataFrame correspond to individuals\n",
    "df = df.T\n",
    "\n",
    "print(\"DataFrame df has been created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What are the maximum and minimum values taken by the “exercised_stock_options” feature used in this example? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the maximum and minimum values taken by “salary”?\n",
    "\n",
    "This feature also had NaNs in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Changes with Feature Scaling\n",
    "\n",
    "**Run** the cell below to do simple feature rescaling, mapping the original ranges of two features onto the interval [0,1]. You should find that some of the points change clusters! You should also find that these clusters are not as stable as in the previous examples. Run the cell multiple times. You should find that the reported clusters are sometimes different, depending on where the centroids were initialized for k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawClusters(n_clusters = 2,\\\n",
    "             feature_list = [feature_1, feature_2],\\\n",
    "             remove_any_zeroes = False,\\\n",
    "             remove_all_zeroes = True,\\\n",
    "             rescale_features = True,\\                     # This has been set to true to do the scaling\n",
    "             x_label = feature_1.replace(\"_\", \" \"),\\\n",
    "             y_label = feature_2.replace(\"_\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Compare this result with the output for the two clusters without scaling. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
