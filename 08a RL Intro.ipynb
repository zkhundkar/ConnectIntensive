{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 08: Introduction to Reinforcement Learning\n",
    "\n",
    "## Part 01: A First Example \n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook contains a version of Tic-Tac-Toe implemented by Wesley Tansy, based on the Sutton and Barto book. We will walk through parts of the code and get an understanding of how the code is structured. This should help you when you review the code provided for the smartcab project.\n",
    "\n",
    "The first five or six cells contain all the code we need to set up the environment and the agent. To start, please run the cells skip to the next narrative section (**After the Code**) and follow instructions from there. We will review the code later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tansey/rl-tictactoe\n",
    "\n",
    "\"\"\"\n",
    "Reference implementation of the Tic-Tac-Toe value function learning agent described in Chapter 1 of \n",
    "\"Reinforcement Learning: An Introduction\" by Sutton and Barto. The agent contains a lookup table that\n",
    "maps states to values, where initial values are 1 for a win, 0 for a draw or loss, and 0.5 otherwise.\n",
    "At every move, the agent chooses either the maximum-value move (greedy) or, with some probability\n",
    "epsilon, a random move (exploratory); by default epsilon=0.1. The agent updates its value function \n",
    "(the lookup table) after every greedy move, following the equation:\n",
    "    V(s) <- V(s) + alpha * [ V(s') - V(s) ]\n",
    "This particular implementation addresses the question posed in Exercise 1.1:\n",
    "    \n",
    "    What would happen if the RL agent taught itself via self-play?\n",
    "The result is that the agent learns only how to maximize its own potential payoff, without consideration\n",
    "for whether it is playing to a win or a draw. Even more to the point, the agent learns a myopic strategy\n",
    "where it basically has a single path that it wants to take to reach a winning state. If the path is blocked\n",
    "by the opponent, the values will then usually all become 0.5 and the player is effectively moving randomly.\n",
    "Created by Wesley Tansey\n",
    "1/21/2013\n",
    "Code released under the MIT license.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import csv\n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"Successfully imported matplotlib! Version {}\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import matplotlib!\")\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = 0\n",
    "PLAYER_X = 1\n",
    "PLAYER_O = 2\n",
    "DRAW = 3\n",
    "\n",
    "BOARD_FORMAT = \"----------------------------\\n| {0} | {1} | {2} |\\n|--------------------------|\\n| {3} | {4} | {5} |\\n|--------------------------|\\n| {6} | {7} | {8} |\\n----------------------------\"\n",
    "NAMES = [' ', 'X', 'O']\n",
    "def printboard(state):\n",
    "    cells = []\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            cells.append(NAMES[state[i][j]].center(6))\n",
    "    print BOARD_FORMAT.format(*cells)\n",
    "\n",
    "def emptystate():\n",
    "    return [[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY]]\n",
    "\n",
    "def gameover(state):\n",
    "    for i in range(3):\n",
    "        if state[i][0] != EMPTY and state[i][0] == state[i][1] and state[i][0] == state[i][2]:\n",
    "            return state[i][0]\n",
    "        if state[0][i] != EMPTY and state[0][i] == state[1][i] and state[0][i] == state[2][i]:\n",
    "            return state[0][i]\n",
    "    if state[0][0] != EMPTY and state[0][0] == state[1][1] and state[0][0] == state[2][2]:\n",
    "        return state[0][0]\n",
    "    if state[0][2] != EMPTY and state[0][2] == state[1][1] and state[0][2] == state[2][0]:\n",
    "        return state[0][2]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state[i][j] == EMPTY:\n",
    "                return EMPTY\n",
    "    return DRAW\n",
    "\n",
    "def last_to_act(state):\n",
    "    countx = 0\n",
    "    counto = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state[i][j] == PLAYER_X:\n",
    "                countx += 1\n",
    "            elif state[i][j] == PLAYER_O:\n",
    "                counto += 1\n",
    "    if countx == counto:\n",
    "        return PLAYER_O\n",
    "    if countx == (counto + 1):\n",
    "        return PLAYER_X\n",
    "    return -1\n",
    "\n",
    "\n",
    "def enumstates(state, idx, agent):\n",
    "    if idx > 8:\n",
    "        player = last_to_act(state)\n",
    "        if player == agent.player:\n",
    "            agent.add(state)\n",
    "    else:\n",
    "        winner = gameover(state)\n",
    "        if winner != EMPTY:\n",
    "            return\n",
    "        i = idx / 3\n",
    "        j = idx % 3\n",
    "        for val in range(3):\n",
    "            state[i][j] = val\n",
    "            enumstates(state, idx+1, agent)\n",
    "            \n",
    "print \"Done loading cell!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, player, verbose = False, lossval = 0, learning = True):\n",
    "        self.values = {}\n",
    "        self.player = player\n",
    "        self.verbose = verbose\n",
    "        self.lossval = lossval\n",
    "        self.learning = learning\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.99\n",
    "        self.prevstate = None\n",
    "        self.prevscore = 0\n",
    "        self.count = 0\n",
    "        enumstates(emptystate(), 0, self)\n",
    "\n",
    "    def episode_over(self, winner):\n",
    "        self.backup(self.winnerval(winner))\n",
    "        self.prevstate = None\n",
    "        self.prevscore = 0\n",
    "\n",
    "    def action(self, state):\n",
    "        r = random.random()\n",
    "        if r < self.epsilon:\n",
    "            move = self.random(state)\n",
    "            #self.log('>>>>>>> Exploratory action: ' + str(move))\n",
    "        else:\n",
    "            move = self.greedy(state)\n",
    "            #self.log('>>>>>>> Best action: ' + str(move))\n",
    "        state[move[0]][move[1]] = self.player\n",
    "        self.prevstate = self.statetuple(state)\n",
    "        self.prevscore = self.lookup(state)\n",
    "        state[move[0]][move[1]] = EMPTY\n",
    "        return move\n",
    "\n",
    "    def random(self, state):\n",
    "        available = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i][j] == EMPTY:\n",
    "                    available.append((i,j))\n",
    "        return random.choice(available)\n",
    "\n",
    "    def greedy(self, state):\n",
    "        maxval = -50000\n",
    "        maxmove = None\n",
    "        if self.verbose:\n",
    "            cells = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i][j] == EMPTY:\n",
    "                    state[i][j] = self.player\n",
    "                    val = self.lookup(state)\n",
    "                    state[i][j] = EMPTY\n",
    "                    if val > maxval:\n",
    "                        maxval = val\n",
    "                        maxmove = (i, j)\n",
    "                    if self.verbose:\n",
    "                        cells.append('{0:.3f}'.format(val).center(6))\n",
    "                elif self.verbose:\n",
    "                    cells.append(NAMES[state[i][j]].center(6))\n",
    "        if self.verbose:\n",
    "            print BOARD_FORMAT.format(*cells)\n",
    "        self.backup(maxval)\n",
    "        return maxmove\n",
    "\n",
    "    def backup(self, nextval):\n",
    "        if self.prevstate != None and self.learning:\n",
    "            self.values[self.prevstate] += self.alpha * (nextval - self.prevscore)\n",
    "\n",
    "    def lookup(self, state):\n",
    "        key = self.statetuple(state)\n",
    "        if not key in self.values:\n",
    "            self.add(key)\n",
    "        return self.values[key]\n",
    "\n",
    "    def add(self, state):\n",
    "        winner = gameover(state)\n",
    "        tup = self.statetuple(state)\n",
    "        self.values[tup] = self.winnerval(winner)\n",
    "\n",
    "    def winnerval(self, winner):\n",
    "        if winner == self.player:\n",
    "            return 1\n",
    "        elif winner == EMPTY:\n",
    "            return 0.5\n",
    "        elif winner == DRAW:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.lossval\n",
    "\n",
    "    def printvalues(self):\n",
    "        vals = deepcopy(self.values)\n",
    "        for key in vals:\n",
    "            state = [list(key[0]),list(key[1]),list(key[2])]\n",
    "            cells = []\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if state[i][j] == EMPTY:\n",
    "                        state[i][j] = self.player\n",
    "                        cells.append(str(self.lookup(state)).center(3))\n",
    "                        state[i][j] = EMPTY\n",
    "                    else:\n",
    "                        cells.append(NAMES[state[i][j]].center(3))\n",
    "            print BOARD_FORMAT.format(*cells)\n",
    "\n",
    "    def statetuple(self, state):\n",
    "        return (tuple(state[0]),tuple(state[1]),tuple(state[2]))\n",
    "\n",
    "    def log(self, s):\n",
    "        if self.verbose:\n",
    "            print s\n",
    "\n",
    "class Human(object):\n",
    "    def __init__(self, player):\n",
    "        self.player = player\n",
    "\n",
    "    def action(self, state):\n",
    "        printboard(state)\n",
    "        action = raw_input('Your move? ')\n",
    "        return (int(action.split(',')[0]),int(action.split(',')[1]))\n",
    "\n",
    "    def episode_over(self, winner):\n",
    "        if winner == DRAW:\n",
    "            print 'Game over! It was a draw.'\n",
    "        else:\n",
    "            print 'Game over! Winner: Player {0}'.format(winner)\n",
    "\n",
    "def play(agent1, agent2):\n",
    "    state = emptystate()\n",
    "    for i in range(9):\n",
    "        if i % 2 == 0:\n",
    "            move = agent1.action(state)\n",
    "        else:\n",
    "            move = agent2.action(state)\n",
    "        state[move[0]][move[1]] = (i % 2) + 1\n",
    "        winner = gameover(state)\n",
    "        if winner != EMPTY:\n",
    "            return winner\n",
    "    return winner\n",
    "\n",
    "def measure_performance_vs_random(agent1, agent2):\n",
    "    epsilon1 = agent1.epsilon\n",
    "    epsilon2 = agent2.epsilon\n",
    "    agent1.epsilon = 0\n",
    "    agent2.epsilon = 0\n",
    "    agent1.learning = False\n",
    "    agent2.learning = False\n",
    "    r1 = Agent(1)\n",
    "    r2 = Agent(2)\n",
    "    r1.epsilon = 1\n",
    "    r2.epsilon = 1\n",
    "    probs = [0,0,0,0,0,0]\n",
    "    games = 100\n",
    "    for i in range(games):\n",
    "        winner = play(agent1, r2)\n",
    "        if winner == PLAYER_X:\n",
    "            probs[0] += 1.0 / games\n",
    "        elif winner == PLAYER_O:\n",
    "            probs[1] += 1.0 / games\n",
    "        else:\n",
    "            probs[2] += 1.0 / games\n",
    "    for i in range(games):\n",
    "        winner = play(r1, agent2)\n",
    "        if winner == PLAYER_O:\n",
    "            probs[3] += 1.0 / games\n",
    "        elif winner == PLAYER_X:\n",
    "            probs[4] += 1.0 / games\n",
    "        else:\n",
    "            probs[5] += 1.0 / games\n",
    "    agent1.epsilon = epsilon1\n",
    "    agent2.epsilon = epsilon2\n",
    "    agent1.learning = True\n",
    "    agent2.learning = True\n",
    "    return probs\n",
    "\n",
    "def measure_performance_vs_each_other(agent1, agent2):\n",
    "    #epsilon1 = agent1.epsilon\n",
    "    #epsilon2 = agent2.epsilon\n",
    "    #agent1.epsilon = 0\n",
    "    #agent2.epsilon = 0\n",
    "    #agent1.learning = False\n",
    "    #agent2.learning = False\n",
    "    probs = [0,0,0]\n",
    "    games = 100\n",
    "    for i in range(games):\n",
    "        winner = play(agent1, agent2)\n",
    "        if winner == PLAYER_X:\n",
    "            probs[0] += 1.0 / games\n",
    "        elif winner == PLAYER_O:\n",
    "            probs[1] += 1.0 / games\n",
    "        else:\n",
    "            probs[2] += 1.0 / games\n",
    "    #agent1.epsilon = epsilon1\n",
    "    #agent2.epsilon = epsilon2\n",
    "    #agent1.learning = True\n",
    "    #agent2.learning = True\n",
    "    return probs\n",
    "\n",
    "def plot_stats(perf, series):\n",
    "    #series = ['P1-Win', 'P2-Win', 'Draw']\n",
    "    colors = ['r','b','g','c','m','b']\n",
    "    markers = ['+', '.', 'o', '*', '^', 's']\n",
    "    for i in range(1,len(perf)):\n",
    "        plt.plot(perf[0], perf[i], label=series[i-1], color=colors[i-1])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('RL Agent Performance vs. Random Agent\\n({0} loss value, self-play)'.format(p1.lossval))\n",
    "    #plt.title('P1 Loss={0} vs. P2 Loss={1}'.format(p1.lossval, p2.lossval))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(n_trials=1000, _display=False):\n",
    "    p1 = Agent(1, lossval = -1)\n",
    "    p2 = Agent(2, lossval = -1)\n",
    "    r1 = Agent(1, learning = False)\n",
    "    r2 = Agent(2, learning = False)\n",
    "    r1.epsilon = 1\n",
    "    r2.epsilon = 1\n",
    "    series = ['P1-Win','P1-Lose','P1-Draw','P2-Win','P2-Lose','P2-Draw']\n",
    "    #f = open('results.csv', 'wb')\n",
    "    #writer = csv.writer(f)    \n",
    "    #writer.writerow(series)\n",
    "    perf = [[] for _ in range(len(series) + 1)]\n",
    "    for i in range(n_trials):\n",
    "        if i % 10 == 0:\n",
    "            if i % 500 == 0:\n",
    "                print 'Game: {0}'.format(i)\n",
    "            probs = measure_performance_vs_random(p1, p2)\n",
    "            #writer.writerow(probs)\n",
    "            #f.flush()\n",
    "            perf[0].append(i)\n",
    "            for idx,x in enumerate(probs):\n",
    "                perf[idx+1].append(x)\n",
    "        winner = play(p1,p2)\n",
    "        p1.episode_over(winner)\n",
    "        #winner = play(r1,p2)\n",
    "        p2.episode_over(winner)\n",
    "    #f.close()\n",
    "    print \"Training on {} trials complete\".format(n_trials)\n",
    "    if _display:\n",
    "        plot_stats(perf, series)\n",
    "    return (p1, p2)\n",
    "    #plt.savefig('p1loss{0}vsp2loss{1}.png'.format(p1.lossval, p2.lossval))\n",
    "    #plt.savefig('selfplay_random_{0}loss.png'.format(p1.lossval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playgame(p1, p2):\n",
    "    p2.verbose = True\n",
    "    p1 = Human(1)\n",
    "    winner = play(p1,p2)\n",
    "    p1.episode_over(winner)\n",
    "    p2.episode_over(winner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the Code\n",
    "\n",
    "Ok, that was a good bit of code! \n",
    "\n",
    "The idea is that we have an agent that can learn by trial and error. However sometimes this can take a large number of trials. For this simple example, it takes thousands. Also, $ tic-tac-toe $ is a two-player game, so who is the agent going to learn from? We don't want to have to sit there and play thousands of games to train it! So what are our choices?\n",
    "\n",
    "Maybe it could play against itself?! That is exactly what we will do. We will have the smart agent learn how to play by playing against another similar agent that just makes random moves. Will that be sufficient? Let's try and see.\n",
    "\n",
    "These are the steps:\n",
    "1. Train the agent against another agent. The code above includes a function ** train ** that takes upto three arguments.\n",
    "    - Number of games to train on. This does take some time, so lets not get too carried away -- lets stick between 100 and 10000. This is a required argument\n",
    "    - flag to indicate whether the agent should train against another agent picking random moves (default) or one which is also a learner. For this session, we can just stick with the default.\n",
    "    - flag to display how the learning proceeded (we'll look at this together during the review)\n",
    "2. Use another function **playgame** to play against the agent you just trained. To make a move, enter the coordinates of the cell  you want to pick. The first number is the row index 0,1,2 (0 is the top row and 2 is te bottom row). The second number is the column index -- 0 is the left column and 2 is the right one. You would enter your move as 0,0 if you wanted to mark the top left cell. You are player \\#1 and your moves are marked with an 'X'.\n",
    "\n",
    "-` Run the cell below to train the agent. _p1_ will be the agent you play against. Then run the code cell below it to play. \n",
    "- Play a set of games, say 10 and note the your number of wins, losses and draws.\n",
    "- Change the value of $ n $ and repeat a few times, say 3 values, low, middle and high (don't o beyond 10000 as you will be waiting for some time!). \n",
    "\n",
    "As you play, you can use the same combination of winning moves (once you've identified a pattern) to see if the agent \"adjusts\" to your playing skills. What do you notice about the agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "p1,p2 = train(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Playing tic-tac-toe**\n",
    "\n",
    "Run the cell below o play _tic tac toe_ against the trained agent. The agent will allow you to make the first move and mark it with an \"X\". Remeber, to enter your move as \"row,col\", e.g., 0,0 will mark the top left corner with \"X\" and 2,2 will mark the bottom right.\n",
    "\n",
    "The agent prints out two grids after your move. The first shows the expected cumulative return for each cell based on what it has learned so far. The second shows the move it picked (marked as \"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playgame(p1,p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
