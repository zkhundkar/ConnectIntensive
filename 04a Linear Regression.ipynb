{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Intensive - Machine Learning Nanodegree\n",
    "\n",
    "## Week 4. Supervised Learning  - Linear Regression  \n",
    "\n",
    "## Objectives    \n",
    "\n",
    "- Learn basics of linear regression\n",
    "- Numpy\n",
    "- Feature scaling\n",
    "- Identifying multi-colinearity\n",
    "\n",
    "Let us start by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! Version {}\".format(np.version.version))\n",
    "except ImportError:\n",
    "    print(\"Could not import numpy!\")\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! Version {}\".format(pd.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "    \n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(\"Successfully imported matplotlib! Version {}\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import matplotlib!\")\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    print(\"Successfully imported seaborn! Version {}\".format(sns.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import seaborn!\")\n",
    "    \n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first exercise should be familiar by now. We will import a regressor and a metric, then train some data to a linear model.\n",
    "$$ y = w_0 + w_1\\cdot x $$\n",
    "The data will be generated so we'll know what the answer should be. The second part will be calculating the slope ($w_1$) and intercept ($w_0$) using the exact formula. The notebook will guide you through it. \n",
    "The third part will be trying out _gradient-descent_ manually so you get a sense of what goes on in the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quick_plot(X, Y, model):\n",
    "    plt.figure(2)\n",
    "    plt.scatter(X, Y, c=\"blue\", alpha=0.3, s=50.0 )\n",
    "    plt.plot(X, model, c=\"gray\")\n",
    "    plt.ylim(Y.min(), Y.max())\n",
    "    plt.show()\n",
    "    \n",
    "X = np.linspace(0, 10, 100)\n",
    "def f_line(m, b, X):\n",
    "    return X*m + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from our notebook from session 2 -- I am generating some noisy data based on a straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a line with slope m and intercept b (keep them between -1.0 and 1.0)\n",
    "Y=f_line(m=0.2, b=-0.3, X=X);\n",
    "# Assign a value to sigma (between 0.1 and 5.0)\n",
    "sigma = 0.2\n",
    "\n",
    "# r is a vector of random numbers drawn from a normal distribution centered on 0, with a standard deviation of sigma.\n",
    "# quick_plot overlays the \"observed\" noisy data (Y+random values) and the underlying true value (Y).\n",
    "r =  np.random.normal(0, sigma, len(X))\n",
    "quick_plot(X, Y + r, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression using `sklearn`\n",
    "\n",
    "** TO DO:**\n",
    "\n",
    "Use the `LinearRegression` class from the `sklearn.linear_model` package to calculate the slope and intercept of the \"best-fit line\" through our generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lreg = None # TODO\n",
    "\n",
    "#TO DO - Fit the regresor to our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The results of the fit are available through the `coef_` attribute after the model has been fit\n",
    "print(\"Slope is {:.3f}, and intercept is {:.3f}\".format(lreg.coef_[0], lreg.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact formula for the slope and intercept of a linear regression to this data is:\n",
    "$$ \\large w_1 =  {\\Sigma (X-\\bar{X})\\cdot(Y - \\bar{Y}) \\over \\Sigma (X - \\bar{X})^2  }  $$\n",
    "\n",
    "$$ \\large w_0 =  \\bar{Y}  - w_1 \\cdot \\bar{X} $$\n",
    "\n",
    "Each of the sums are over all data points, i.e. $\\Sigma_{i=1}^N$. $\\bar{X}$ and $\\bar{Y}$ are the average values of $X$ and $Y$.\n",
    "\n",
    "**TO DO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The goal here is to calculate the slope and intercept using these formulas and numpy functions you are familiar with\n",
    "# Hints: 1. Calcuate the mean of X and Y\n",
    "#        2. Calculate X - X.mean\n",
    "#        3. Calculate the slope (w1). The division is over scalar values so you need to sum over all data points before dividing \n",
    "slope = 0  # TODO - implement this\n",
    "\n",
    "# Calulate the intercept.\n",
    "intercept = 0 # TO DO  - implement this\n",
    "\n",
    "print(\"Slope is {:.3f}, and intercept is {:.3f}\".format(slope, intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third part, we are going to do a walk-through of gradient descent in one dimension. The method works because the error function is *convex*, i.e., looks bowl-shaped and calculus tells us that the quickest way to get to the bottom is following the path of steepest descent. `Steepest descent` is basically another term for `direction in which the change in error is largest`. In the algorithms we will use, the \"slope\" (or gradient in multi-dimensions) isn't calculated exactly this way, but the principle is similar.\n",
    "\n",
    "For this part to work without errors, the arguments should have the correct shape. X and Y should be numpy array of shape (100, 2) and (100, 1) respectively. W should be a numpy array of shape (1, 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate mean squared error \n",
    "# Y - np.array(N, 1) - ground truth\n",
    "# Y_pred - np.array(N, 1) - predictions\n",
    "def mse(Y, Y_pred):\n",
    "    return np.sum((Y-Y_pred)**2)/Y.shape[0]\n",
    "\n",
    "# A very simple gradient function \n",
    "#     Calculate the gradient by comparing the difference in MSE for small \n",
    "#      changes in the two components of W\n",
    "def get_gradient(X, Y, W):\n",
    "    MSE = mse(Y, np.dot(X, W))\n",
    "    step = 1.0e-5\n",
    "    W_test = W.copy()\n",
    "    W_test = W_test + np.array([step, 0]).reshape(2, 1)\n",
    "    delta0 = (MSE - mse(Y, np.dot(X, W_test)))/step\n",
    "    W_test = W_test + np.array([-step, step]).reshape(2, 1)\n",
    "    delta1 = (MSE - mse(Y, np.dot(X, W_test)))/step\n",
    "    return np.array([delta0, delta1])\n",
    "\n",
    "def update_weights(W, gradient, lrate=0.001):\n",
    "    return W + (gradient * lrate).reshape(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up the numpy arrays we will need for this exercise. \n",
    "We will start with a guess for the slope and intercept as weights. Notice that the way I have defined X_train, weights[0][0] will be the intercept and weights[1][0] will be the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.ones(200).reshape(100,2)\n",
    "X_train[:,1] = X\n",
    "X_train = X_train.reshape(100, 2)\n",
    "\n",
    "weights = np.array([1.0, 0.5]).reshape(2, 1)\n",
    "Y=Y.reshape(100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO:** The next step is to choose a `learning rate` (lrate). Choosing this is very important in getting good results. For this part of the exercise, try three different values of lrate and run the cell below repeatedly to see if you can get the slope and intercept close to the values you expect. Write down what you observe for each lrate value you tried below, along with approximately how many iterations it took to reach a value close enough to satisfy you.\n",
    "\n",
    "If you run into trouble like the values not getting better, reset the problem by running the cell above again.\n",
    "\n",
    "_HINT:_ Small `lrate` will take a long time to converge and large `lrate` will not converge at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrate = 0.02\n",
    "for i in range(100):\n",
    "    grad = get_gradient(X_train, Y, weights)\n",
    "    delta = update_weights(weights, grad, lrate)\n",
    "    weights = weights + (grad*lrate).reshape(2,1)\n",
    "print \"MSE = {:.5f}, Slope = {:.3f} and intercept = {:.3f}\"\\\n",
    "        .format(mse(np.dot(X_train, weights), Y), weights[1][0], weights[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
